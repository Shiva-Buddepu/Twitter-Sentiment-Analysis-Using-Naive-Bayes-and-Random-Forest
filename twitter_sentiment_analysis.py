# -*- coding: utf-8 -*-
"""Twitter Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CGN6vuGqZompFiSgzX8wHInXFoCz_cWH

##Import Needed Modules
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

import spacy

"""##EDA"""

# Read the dataset with name "Emotion_classify_Data.csv" and store it in a variable df
columns = ['id','country','Label','Text']
df = pd.read_csv("twitter_training.csv", names=columns)

# Print the shape of dataframe
print(df.shape)

# Print top 5 rows
df.head(5)

df.info()

# Check the distribution of Emotion
df['Label'].value_counts()

# Show sample
for i in range(5):
    print(f"{i+1}: {df['Text'][i]} -> {df['Label'][i]}")

"""##Preprocessing"""

#Drop nan values
df.dropna(inplace=True)

import spacy

# Load spaCy model with only tokenizer + tagger + lemmatizer
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

def preprocess_texts(text_list, batch_size=1000):
    processed = []
    for doc in nlp.pipe(text_list, batch_size=batch_size):
        tokens = [token.lemma_ for token in doc
                  if not token.is_stop and not token.is_punct]
        processed.append(" ".join(tokens))
    return processed

# Apply on your dataframe column
df["Preprocessed Text"] = preprocess_texts(df["Text"].tolist())

df

#Encoding target column
le_model = LabelEncoder()
df['Label'] = le_model.fit_transform(df['Label'])

df.head(5)

#Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(df['Preprocessed Text'], df['Label'],
                                                    test_size=0.2, random_state=42, stratify=df['Label'])

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)

"""##Naive Bayes Model"""

# Create classifier
clf = Pipeline([
    ('vectorizer_tri_grams', TfidfVectorizer()),
    ('naive_bayes', (MultinomialNB()))
])

# Model training
clf.fit(X_train, y_train)

# Get prediction
y_pred = clf.predict(X_test)

# Print score
print(accuracy_score(y_test, y_pred))

# Print classification report
print(classification_report(y_test, y_pred))

"""##Random Forest"""

clf = Pipeline([
    ('vectorizer_tri_grams', TfidfVectorizer()),
    ('naive_bayes', (RandomForestClassifier()))
])

clf.fit(X_train, y_train)

# Get the predictions for X_test and store it in y_pred
y_pred = clf.predict(X_test)

# Print Accuracy
print(accuracy_score(y_test, y_pred))

# Print the classfication report
print(classification_report(y_test, y_pred))

"""##Test Model"""

test_df = pd.read_csv('twitter_validation.csv', names=columns)
test_df.head()

test_text = test_df['Text'][10]
print(f"{test_text} ===> {test_df['Label'][10]}")

#Apply preprocess

test_text_processed = [preprocess(test_text)]
test_text_processed

#Get Prediction

test_text = clf.predict(test_text_processed)

#Output

classes = ['Irrelevant', 'Natural', 'Negative', 'Positive']

print(f"True Label: {test_df['Label'][10]}")
print(f'Predict Label: {classes[test_text[0]]}')